{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurance = 10\n",
    "# Word2Vec parameters\n",
    "size_word2vec = 100\n",
    "min_count_word2vec = 10\n",
    "# NN s general parameters\n",
    "batch_size = 512\n",
    "epochs = 50\n",
    "test_size=0.25\n",
    "# LSTM s parameters\n",
    "lstm_size = 512\n",
    "lstm_dropout = 0.5\n",
    "lstm_recurrent_dropout = 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import cleaned_data CSV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"C:/Users/ThÃ©o/Documents/twitter_sentiment_analysis/data/cleaned_data.csv\",\n",
    "    # \"C:/Users/HENAFF/Documents/Cours Polytech/S9 en Roumanie/Machine Learning - ML/data/mid_cleaned_data.csv\",\n",
    "    # nrows=20000,\n",
    "    encoding='latin-1')\n",
    "df['clean_text'] = df.clean_text.astype(str)\n",
    "\n",
    "dict_word = {}\n",
    "sentences = []\n",
    "for items in df['clean_text'].iteritems():\n",
    "    words = items[1].split(\" \")\n",
    "    sentences.append(words)\n",
    "    for word in words:\n",
    "        if word in dict_word:\n",
    "            dict_word[word] += 1\n",
    "        else:\n",
    "            dict_word[word] = 1  # dictionary['UNK']\n",
    "\n",
    "\n",
    "cleaned_dict_word = [k for k,c in dict_word.items() if c >= min_occurance]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Word2Vec model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define functions to create the word2vec weight matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_weight_matrix(vocab):\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "\n",
    "    for w, i in vocab.items():\n",
    "        # The word_index contains a token for all words of the training data so we need to limit that\n",
    "        if i < vocab_size:\n",
    "            try:\n",
    "                vect = model.wv.get_vector(w)\n",
    "                weight_matrix[i] = vect\n",
    "            # Check if the word from the training data occurs in the GloVe word embeddings\n",
    "            # Otherwise the vector is kept with only zeros\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            break\n",
    "    return weight_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert text to vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in df['clean_text']])\n",
    "# fit the tokenizer on the documents\n",
    "tk = Tokenizer(lower=True)\n",
    "tk.fit_on_texts(df['clean_text'].values)\n",
    "\n",
    "X_seq = tk.texts_to_sequences(df['clean_text'].values)\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_length, padding='post')  # maxlen must be equal to maxword\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tk.word_index) + 1\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "# get vectors in the right order\n",
    "# embedding_vectors = get_weight_matrix(raw_embedding, tk.word_index)\n",
    "embedding_vectors = get_weight_matrix(tk.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, size_word2vec, input_length=max_length, trainable=False, weights=[embedding_vectors])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split test/train data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, df['polarity'].values, test_size=test_size, random_state=1, shuffle=True)\n",
    "\n",
    "X_train1 = X_train[batch_size:]\n",
    "y_train1 = y_train[batch_size:]\n",
    "X_valid = X_train[:batch_size]\n",
    "y_valid = y_train[:batch_size]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model description"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(lstm_size, dropout=lstm_dropout, recurrent_dropout=lstm_recurrent_dropout, return_sequences=True))\n",
    "model.add(LSTM(lstm_size, dropout=lstm_dropout, recurrent_dropout=lstm_recurrent_dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train1, y_train1, shuffle=True, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate the model performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy : \", scores[1])\n",
    "\n",
    "\n",
    "loss = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "accuracy = history.history['accuracy']\n",
    "accuracy_val = history.history['val_accuracy']\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b--', label='validation loss')\n",
    "plt.plot(epochs, accuracy, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, accuracy_val, 'r--', label='validation accuracy')\n",
    "\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
